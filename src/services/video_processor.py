"""
å½±ç‰‡è™•ç†æ¨¡çµ„
è™•ç† YouTube å½±ç‰‡ä¸‹è¼‰ã€å­—å¹•è™•ç†ã€éŸ³è¨Šè½‰éŒ„ç­‰åŠŸèƒ½
ä½¿ç”¨ faster-whisper é€²è¡Œ VRAM å„ªåŒ–
"""
import os
import subprocess
import tempfile
import streamlit as st
from faster_whisper import WhisperModel
from src.core.config import (
    YT_DLP_PATH, FFMPEG_PATH, AUDIO_FILENAME, SUBTITLE_FILENAME, 
    TRANSCRIPT_FILENAME, SUBTITLE_LANGUAGES, SUPPORTED_LANGUAGES
)


class VideoProcessor:
    """å½±ç‰‡è™•ç†å™¨"""
    
    @staticmethod
    def check_device_availability():
        """æª¢æŸ¥ç³»çµ±å¯ç”¨çš„é‹ç®—è¨­å‚™"""
        try:
            import torch
            
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "æœªçŸ¥ GPU"
                cuda_version = torch.version.cuda
                return f"GPU å¯ç”¨ ({gpu_name}, {gpu_count} å€‹è¨­å‚™, CUDA {cuda_version})"
            else:
                pytorch_version = torch.__version__
                if '+cpu' in pytorch_version:
                    return "åƒ… CPU å¯ç”¨ (å®‰è£çš„æ˜¯ CPU-only ç‰ˆæœ¬çš„ PyTorch)"
                else:
                    return "åƒ… CPU å¯ç”¨ (CUDA ä¸å¯ç”¨ - å¯èƒ½éœ€è¦å®‰è£ NVIDIA é©…å‹•æˆ–é‡æ–°å®‰è£æ”¯æ´ CUDA çš„ PyTorch)"
        except ImportError:
            return "PyTorch æœªå®‰è£ï¼Œä½¿ç”¨ Whisper é è¨­è¨­å®š"
        except Exception as e:
            return f"è¨­å‚™æª¢æŸ¥å¤±æ•—ï¼š{e}"
    
    @staticmethod
    def check_and_download_subtitles(youtube_url, cookie_file=None):
        """æª¢æŸ¥ä¸¦ä¸‹è¼‰ CC å­—å¹•ï¼Œå„ªå…ˆé †åºï¼šä¸­æ–‡ > è‹±æ–‡ > å…¶ä»–èªè¨€"""
        st.write("ğŸ” æ­¥é©Ÿ 1/6: æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ CC å­—å¹•...")
        
        check_command = [YT_DLP_PATH, "--list-subs", youtube_url]
        if cookie_file:
            check_command.extend(["--cookies", cookie_file])
        
        try:
            process = subprocess.Popen(check_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, creationflags=subprocess.CREATE_NO_WINDOW)
            stdout, stderr = process.communicate()
            
            if process.returncode != 0:
                st.write("âš ï¸ ç„¡æ³•ç²å–å­—å¹•åˆ—è¡¨ï¼Œå°‡ä½¿ç”¨èªéŸ³è½‰æ–‡å­—...")
                return False
            
            subtitle_info = stdout.decode('utf-8', errors='ignore')
            
            if "Available subtitles" not in subtitle_info and "Available automatic captions" not in subtitle_info:
                st.write("â„¹ï¸ æ­¤å½±ç‰‡æ²’æœ‰å¯ç”¨çš„å­—å¹•ï¼Œå°‡ä½¿ç”¨èªéŸ³è½‰æ–‡å­—...")
                return False
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯¦éš›çš„å­—å¹•èªè¨€
            has_subtitles = False
            for line in subtitle_info.split('\n'):
                if any(lang in line for lang in SUPPORTED_LANGUAGES) and ('vtt' in line or 'srt' in line):
                    has_subtitles = True
                    break
            
            if not has_subtitles:
                st.write("âš ï¸ é›–ç„¶æœ‰å­—å¹•å€å¡Šï¼Œä½†æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„å­—å¹•èªè¨€ï¼Œå°‡ä½¿ç”¨èªéŸ³è½‰æ–‡å­—...")
                return False
            
            st.write("âœ… æ‰¾åˆ°å¯ç”¨å­—å¹•ï¼Œæ­£åœ¨ä¸‹è¼‰...")
            return VideoProcessor._download_subtitles(youtube_url, cookie_file)
            
        except FileNotFoundError:
            st.error(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° yt-dlpã€‚è·¯å¾‘: {YT_DLP_PATH}")
            return False
        except Exception as e:
            st.error(f"âŒ æª¢æŸ¥å­—å¹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    @staticmethod
    def _download_subtitles(youtube_url, cookie_file):
        """ä¸‹è¼‰å­—å¹•çš„å…§éƒ¨æ–¹æ³•"""
        # å˜—è©¦ä¸‹è¼‰å­—å¹•
        for lang in SUBTITLE_LANGUAGES:
            st.write(f"ğŸ“¥ å˜—è©¦ä¸‹è¼‰ {lang} å­—å¹•...")
            download_command = [
                YT_DLP_PATH, "--write-sub", "--sub-lang", lang, 
                "--skip-download", "--sub-format", "vtt",
                "-o", "_temp_subtitle", youtube_url
            ]
            if cookie_file:
                download_command.extend(["--cookies", cookie_file])
            
            try:
                process = subprocess.Popen(download_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, creationflags=subprocess.CREATE_NO_WINDOW)
                stdout, stderr = process.communicate()
                
                possible_files = [
                    f"_temp_subtitle.{lang}.vtt",
                    "_temp_subtitle.vtt"
                ]
                
                for subtitle_file in possible_files:
                    if os.path.exists(subtitle_file):
                        if os.path.getsize(subtitle_file) > 0:
                            os.rename(subtitle_file, SUBTITLE_FILENAME)
                            st.write(f"âœ… æˆåŠŸä¸‹è¼‰ {lang} å­—å¹•")
                            return True
                        else:
                            st.write(f"âš ï¸ {lang} å­—å¹•æª”æ¡ˆç‚ºç©ºï¼Œå˜—è©¦ä¸‹ä¸€å€‹èªè¨€...")
                            os.remove(subtitle_file)
                            
            except Exception as e:
                st.write(f"âŒ ä¸‹è¼‰ {lang} å­—å¹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # å˜—è©¦ä¸‹è¼‰è‡ªå‹•å­—å¹•
        return VideoProcessor._download_auto_subtitles(youtube_url, cookie_file)
    
    @staticmethod
    def _download_auto_subtitles(youtube_url, cookie_file):
        """ä¸‹è¼‰è‡ªå‹•å­—å¹•"""
        st.write("ğŸ“¥ å˜—è©¦ä¸‹è¼‰è‡ªå‹•ç”Ÿæˆå­—å¹•...")
        download_command = [
            YT_DLP_PATH, "--write-auto-sub", "--skip-download", 
            "--sub-format", "vtt", "-o", "_temp_subtitle", youtube_url
        ]
        if cookie_file:
            download_command.extend(["--cookies", cookie_file])
        
        try:
            process = subprocess.Popen(download_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, creationflags=subprocess.CREATE_NO_WINDOW)
            stdout, stderr = process.communicate()
            
            for file in os.listdir('.'):
                if file.startswith('_temp_subtitle') and file.endswith('.vtt'):
                    if os.path.getsize(file) > 0:
                        os.rename(file, SUBTITLE_FILENAME)
                        st.write("âœ… æˆåŠŸä¸‹è¼‰è‡ªå‹•ç”Ÿæˆå­—å¹•")
                        return True
                    else:
                        st.write("âš ï¸ è‡ªå‹•å­—å¹•æª”æ¡ˆç‚ºç©º...")
                        os.remove(file)
                    
        except Exception as e:
            st.write(f"âŒ ä¸‹è¼‰è‡ªå‹•å­—å¹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        st.write("â„¹ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„å­—å¹•ï¼Œå°‡ä½¿ç”¨èªéŸ³è½‰æ–‡å­—...")
        return False
    
    @staticmethod
    def download_audio(youtube_url, cookie_file=None):
        """ä½¿ç”¨ yt-dlp ä¸‹è¼‰éŸ³è¨Š"""
        st.write("ğŸµ æ­¥é©Ÿ 2/6: é–‹å§‹ä¸‹è¼‰éŸ³è¨Š...")
        
        # æª¢æŸ¥ ffmpeg æ˜¯å¦å¯ç”¨
        ffmpeg_available = False
        try:
            subprocess.run([FFMPEG_PATH, "-version"], 
                          stdout=subprocess.PIPE, 
                          stderr=subprocess.PIPE, 
                          creationflags=subprocess.CREATE_NO_WINDOW,
                          check=True)
            ffmpeg_available = True
            st.info(f"âœ… æ‰¾åˆ° FFmpeg: {FFMPEG_PATH}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            st.warning(f"âš ï¸ æœªæ‰¾åˆ° FFmpeg ({FFMPEG_PATH})ï¼Œå°‡ä½¿ç”¨ yt-dlp å…§å»ºè½‰æ›åŠŸèƒ½")
        
        # å»ºç«‹ä¸‹è¼‰å‘½ä»¤
        command = [
            YT_DLP_PATH, "-x", "--audio-format", "mp3",
            "-o", AUDIO_FILENAME, youtube_url
        ]
        
        # åªæœ‰åœ¨ ffmpeg å¯ç”¨æ™‚æ‰æ·»åŠ  ffmpeg-location åƒæ•¸
        if ffmpeg_available:
            command.extend(["--ffmpeg-location", FFMPEG_PATH])
        
        if cookie_file:
            command.extend(["--cookies", cookie_file])

        try:
            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, creationflags=subprocess.CREATE_NO_WINDOW)
            stdout, stderr = process.communicate()
            if process.returncode != 0:
                error_msg = stderr.decode('utf-8', errors='ignore')
                st.error(f"âŒ yt-dlp ä¸‹è¼‰å¤±æ•—: {error_msg}")
                return False
            st.success(f"âœ… éŸ³è¨Šå·²æˆåŠŸä¸‹è¼‰ç‚º {AUDIO_FILENAME}")
            return True
        except FileNotFoundError:
            st.error(f"âŒ æ‰¾ä¸åˆ° yt-dlpã€‚è·¯å¾‘: {YT_DLP_PATH}")
            return False
        except Exception as e:
            st.error(f"âŒ ä¸‹è¼‰æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
            return False
    
    @staticmethod
    def get_model_device(model):
        """ç²å–æ¨¡å‹å¯¦éš›ä½¿ç”¨çš„è¨­å‚™"""
        try:
            if hasattr(model, 'device'):
                device = str(model.device)
                return f"GPU ({device})" if 'cuda' in device.lower() else f"CPU ({device})"
            else:
                return "ç„¡æ³•ç¢ºå®šè¨­å‚™"
        except Exception as e:
            return f"è¨­å‚™æª¢æŸ¥å¤±æ•—ï¼š{e}"
    
    @staticmethod
    def transcribe_audio(model_name="base"):
        """ä½¿ç”¨ faster-whisper å°‡éŸ³è¨Šè½‰ç‚ºé€å­—ç¨¿ï¼ˆæœ€å¤§ VRAM æ¶ˆè€— + æœ€é«˜é€Ÿåº¦ï¼‰"""
        st.write("ğŸ”¥ æ­¥é©Ÿ 3/6: æœ€å¤§ VRAM æ¶ˆè€—æ¨¡å¼ - é–‹å§‹é€²è¡ŒèªéŸ³è½‰æ–‡å­— (base æ¨¡å‹ + æ¥µé™é€Ÿåº¦é…ç½®)...")
        if not os.path.exists(AUDIO_FILENAME):
            st.error(f"âŒ æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ {AUDIO_FILENAME}")
            return False

        try:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text(f"æ­£åœ¨è¼‰å…¥ Faster-Whisper {model_name} é«˜æ•ˆèƒ½æ¨¡å‹...")
            progress_bar.progress(10)
            
            device_info = VideoProcessor.check_device_availability()
            st.info(f"ğŸ–¥ï¸ ç³»çµ±é‹ç®—è¨­å‚™ç‹€æ…‹ï¼š{device_info}")
            
            # ç¢ºå®šä½¿ç”¨çš„è¨­å‚™å’Œç²¾åº¦è¨­å®š
            try:
                import torch
                cuda_available = torch.cuda.is_available()
                if cuda_available:
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    st.info(f"ğŸ¯ GPU è¨˜æ†¶é«”ç¸½å®¹é‡: {gpu_memory:.1f} GB")
                    # æ¸…ç†ä¹‹å‰çš„ GPU è¨˜æ†¶é«”
                    torch.cuda.empty_cache()
                    initial_memory = torch.cuda.memory_allocated(0) / (1024**3)
                    st.info(f"ğŸ“Š åˆå§‹ GPU è¨˜æ†¶é«”ä½¿ç”¨: {initial_memory:.3f} GB")
            except ImportError:
                cuda_available = False
            
            if cuda_available:
                device = "cuda"
                compute_type = "float16"  # ä½¿ç”¨ float16 å¢åŠ  VRAM ä½¿ç”¨é‡
                st.info("ğŸš€ ä½¿ç”¨ GPU é«˜ VRAM æ¨¡å¼ï¼šfloat16 ç²¾åº¦ (å¢åŠ è¨˜æ†¶é«”ä½¿ç”¨)")
            else:
                device = "cpu"
                compute_type = "int8"
                st.info("ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼ï¼šint8 ç²¾åº¦")
            
            progress_bar.progress(30)
            
            # è¨­å®šæ¨¡å‹å¿«å–ç›®éŒ„
            cache_dir = os.path.join(tempfile.gettempdir(), "faster_whisper_models")
            os.makedirs(cache_dir, exist_ok=True)
            
            status_text.text(f"æ­£åœ¨åˆå§‹åŒ– {model_name} æ¨¡å‹...")
            progress_bar.progress(40)
            
            try:
                # ç¢ºå®šå¤šåŸ·è¡Œç·’è¨­å®š - é«˜æ•ˆèƒ½é…ç½®
                import multiprocessing
                
                # ç²å– CPU æ ¸å¿ƒæ•¸é‡
                cpu_count = multiprocessing.cpu_count()
                
                if device == "cuda":
                    # GPU æœ€å¤§ VRAM æ¨¡å¼ï¼šæ¥µé™ä¸¦è¡Œé…ç½®
                    cpu_threads = cpu_count  # ä½¿ç”¨æ‰€æœ‰ CPU åŸ·è¡Œç·’è¼”åŠ© GPU
                    num_workers = 32  # å¤§å¹…å¢åŠ å·¥ä½œåŸ·è¡Œç·’æ•¸æ¶ˆè€—æ›´å¤š VRAM
                    st.info(f"ğŸ”¥ GPU æœ€å¤§ VRAM æ¨¡å¼å¤šåŸ·è¡Œç·’ï¼š{cpu_threads} CPU åŸ·è¡Œç·’ï¼Œ{num_workers} å·¥ä½œåŸ·è¡Œç·’")
                    st.info("ğŸ¯ ç›®æ¨™ï¼šä½¿ç”¨ base æ¨¡å‹ + æ¥µé™ä¸¦è¡Œä¾†æ¶ˆè€—æ‰€æœ‰ VRAM")
                else:
                    # CPU æ¨¡å¼ï¼šæœ€å¤§åŒ–åˆ©ç”¨å¤šæ ¸å¿ƒ
                    cpu_threads = cpu_count  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ¸å¿ƒ
                    num_workers = min(2, max(1, cpu_count // 4))  # æ ¹æ“šæ ¸å¿ƒæ•¸æ±ºå®š worker æ•¸é‡
                    st.info(f"ğŸ’» CPU æ¨¡å¼å¤šåŸ·è¡Œç·’è¨­å®šï¼š{cpu_threads} CPU åŸ·è¡Œç·’ï¼Œ{num_workers} å·¥ä½œåŸ·è¡Œç·’")
                
                # å»ºç«‹ faster-whisper æ¨¡å‹ï¼ˆé«˜ VRAM é…ç½®ï¼‰
                model = WhisperModel(
                    model_name, 
                    device=device,
                    compute_type=compute_type,
                    cpu_threads=cpu_threads,  # è¨­å®š CPU åŸ·è¡Œç·’æ•¸
                    num_workers=num_workers,  # è¨­å®šå·¥ä½œåŸ·è¡Œç·’æ•¸
                    download_root=cache_dir,
                    local_files_only=False  # å…è¨±ä¸‹è¼‰æœ€æ–°æ¨¡å‹
                )
                
                # æª¢æŸ¥æ¨¡å‹è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨
                if device == "cuda":
                    try:
                        torch.cuda.synchronize()  # åŒæ­¥ GPU æ“ä½œ
                        after_load_memory = torch.cuda.memory_allocated(0) / (1024**3)
                        memory_increase = after_load_memory - initial_memory
                        st.info(f"ğŸ“ˆ æ¨¡å‹è¼‰å…¥å¾Œ GPU è¨˜æ†¶é«”: {after_load_memory:.3f} GB (+{memory_increase:.3f} GB)")
                        
                        # é¡¯ç¤º VRAM ä½¿ç”¨ç‡
                        vram_usage_percent = (after_load_memory / gpu_memory) * 100
                        st.info(f"ğŸ’¾ VRAM ä½¿ç”¨ç‡: {vram_usage_percent:.1f}%")
                        
                        st.info("ğŸ”¥ **é«˜ VRAM æ¨¡å¼èªªæ˜**:")
                        st.info(f"   â€¢ ä½¿ç”¨ {model_name} æ¨¡å‹ (æ›´é«˜æº–ç¢ºåº¦)")
                        st.info("   â€¢ float16 ç²¾åº¦ (å¢åŠ è¨˜æ†¶é«”ä½¿ç”¨)")
                        st.info(f"   â€¢ {num_workers} å€‹å·¥ä½œåŸ·è¡Œç·’ (æé«˜ä¸¦è¡Œåº¦)")
                        st.info("   â€¢ é æœŸæ•ˆèƒ½æå‡ 2-5 å€ ğŸš€")
                    except:
                        st.info("ğŸ“Š GPU è¨˜æ†¶é«”è³‡è¨Šç²å–ä¸­...")
                
                st.success(f"âœ… æˆåŠŸè¼‰å…¥ {model_name} é«˜æ•ˆèƒ½æ¨¡å‹")
                
            except Exception as e:
                st.warning(f"è¼‰å…¥ {model_name} æ¨¡å‹å¤±æ•—: {e}")
                st.info("ğŸ”„ å˜—è©¦ä½¿ç”¨ base æ¨¡å‹...")
                
                # å‚™ç”¨æ¨¡å‹ä¹Ÿä½¿ç”¨é«˜ VRAM è¨­å®š
                model = WhisperModel(
                    "base", 
                    device=device, 
                    compute_type=compute_type,
                    cpu_threads=cpu_threads,
                    num_workers=num_workers,
                    download_root=cache_dir
                )
                st.success("âœ… æˆåŠŸè¼‰å…¥ base å‚™ç”¨æ¨¡å‹ï¼ˆé«˜ VRAM ç‰ˆæœ¬ï¼‰")
            
            progress_bar.progress(50)
            status_text.text("é–‹å§‹è½‰éŒ„éŸ³è¨Š...")
            
            # è¨­å®š FFmpeg è·¯å¾‘
            original_path = os.environ.get('PATH', '')
            internal_dir = os.path.dirname(FFMPEG_PATH)
            if internal_dir not in original_path:
                os.environ['PATH'] = f"{internal_dir};{original_path}"
                st.info(f"ğŸ”§ å·²è¨­å®š FFmpeg è·¯å¾‘ï¼š{internal_dir}")
            
            # é€²è¡Œæ¥µé™æ•ˆèƒ½è½‰éŒ„
            st.info(f"ğŸ¯ é–‹å§‹è½‰éŒ„ï¼š{model_name} æ¨¡å‹ï¼ˆæ¥µé™ VRAM æ¶ˆè€—æ¨¡å¼ï¼‰")
            st.info(f"ğŸ–¥ï¸ è¨­å‚™ï¼š{device} | ç²¾åº¦ï¼š{compute_type} | å·¥ä½œåŸ·è¡Œç·’ï¼š{num_workers if device == 'cuda' else 'N/A'}")
            
            # ä½¿ç”¨æ¥µé™æ•ˆèƒ½åƒæ•¸é€²è¡Œè½‰éŒ„ï¼ˆæœ€å¤§åŒ–ä¸¦è¡Œè™•ç†ï¼‰
            segments, info = model.transcribe(
                AUDIO_FILENAME, 
                language="zh",
                beam_size=1,  # ä½¿ç”¨æœ€å°æŸæœç´¢æ›å–æœ€å¤§é€Ÿåº¦
                temperature=0.0,  # ç¢ºå®šæ€§è½‰éŒ„
                vad_filter=True,  # èªéŸ³æ´»å‹•æª¢æ¸¬
                vad_parameters=dict(
                    min_silence_duration_ms=100,  # æœ€å°éœéŸ³æŒçºŒæ™‚é–“ï¼ˆæ¥µå¿«éŸ¿æ‡‰ï¼‰
                    threshold=0.1,  # éå¸¸æ•æ„Ÿçš„ VAD é–¾å€¼
                    min_speech_duration_ms=100,  # æœ€å°èªéŸ³æŒçºŒæ™‚é–“
                    max_speech_duration_s=30  # çŸ­èªéŸ³æ®µï¼ˆæœ€å¤§åŒ–ä¸¦è¡Œï¼‰
                ),
                word_timestamps=False,  # é—œé–‰ç²¾ç¢ºæ™‚é–“æˆ³ä»¥ç¯€çœè¨˜æ†¶é«”çµ¦ä¸¦è¡Œè™•ç†
                compression_ratio_threshold=10.0,  # å¯¬é¬†çš„å£“ç¸®æ¯”é–¾å€¼
                log_prob_threshold=-2.0,  # å¯¬é¬†çš„æ©Ÿç‡é–¾å€¼
                no_speech_threshold=0.1,  # æ¥µæ•æ„Ÿçš„ç„¡èªéŸ³æª¢æ¸¬
                condition_on_previous_text=False,  # é—œé–‰å‰æ–‡é æ¸¬ä»¥åŠ é€Ÿ
                initial_prompt=None  # ä¸ä½¿ç”¨åˆå§‹æç¤ºä»¥æ¸›å°‘è¨˜æ†¶é«”é–‹éŠ·
            )
            
            if device == "cuda":
                st.info("ğŸš€ GPU æ¥µé™ VRAM å¤šåŸ·è¡Œç·’è½‰éŒ„é€²è¡Œä¸­...")
                st.info("   â€¢ 32 å€‹ä¸¦è¡Œå·¥ä½œåŸ·è¡Œç·’åŒæ™‚é‹ä½œ ğŸ’¥")
                st.info("   â€¢ æœ€å¤§åŒ– GPU è¨˜æ†¶é«”ä½¿ç”¨ä»¥ç²å¾—æ¥µé™é€Ÿåº¦")
                st.info("   â€¢ é æœŸæ¶ˆè€—å¤§éƒ¨åˆ†å¯ç”¨ VRAM ğŸ’¾")
            else:
                st.info("ğŸ’» CPU å¤šåŸ·è¡Œç·’è½‰éŒ„é€²è¡Œä¸­...")
            
            progress_bar.progress(80)
            status_text.text("æ­£åœ¨æ•´ç†è½‰éŒ„çµæœ...")
            
            # æ”¶é›†è½‰éŒ„æ–‡å­—
            transcript_text = ""
            segment_count = 0
            for segment in segments:
                transcript_text += segment.text + " "
                segment_count += 1
            
            # å„²å­˜çµæœ
            with open(TRANSCRIPT_FILENAME, "w", encoding="utf-8") as f:
                f.write(transcript_text.strip())
                
            progress_bar.progress(100)
            status_text.text("è½‰éŒ„å®Œæˆï¼")
            
            # é¡¯ç¤ºçµæœçµ±è¨ˆ
            st.success(f"âœ… é€å­—ç¨¿å·²æˆåŠŸå„²å­˜ç‚º {TRANSCRIPT_FILENAME}")
            st.info(f"ğŸ“Š è½‰éŒ„çµ±è¨ˆï¼š{segment_count} å€‹ç‰‡æ®µï¼Œèªè¨€: {info.language}")
            
            # é¡¯ç¤ºå¤šåŸ·è¡Œç·’æ•ˆèƒ½èªªæ˜
            if device == "cuda":
                st.info("ğŸ”¥ **GPU å¤šåŸ·è¡Œç·’åŠ é€ŸæˆåŠŸ** - æ‚¨å¯ä»¥åœ¨å·¥ä½œç®¡ç†å“¡ä¸­çœ‹åˆ° GPU ä½¿ç”¨ç‡")
            else:
                st.info(f"âš¡ **CPU å¤šåŸ·è¡Œç·’åŠ é€ŸæˆåŠŸ** - ä½¿ç”¨äº† {cpu_threads} å€‹ CPU åŸ·è¡Œç·’")
            
            return True
            
        except Exception as e:
            st.error(f"âŒ Faster-Whisper è½‰éŒ„å¤±æ•—: {e}")
            import traceback
            st.error(f"è©³ç´°éŒ¯èª¤è³‡è¨Šï¼š{traceback.format_exc()}")
            return False
