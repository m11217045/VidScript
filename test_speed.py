#!/usr/bin/env python3
"""
VidScript Faster-Whisper é€Ÿåº¦æ¸¬è©¦
æ¸¬è©¦ç•¶å‰é …ç›®é…ç½®çš„å¯¦éš›æ€§èƒ½
"""

import os
import sys
import time
import tempfile
import wave
import numpy as np

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def create_test_audio(duration_seconds):
    """å‰µå»ºæŒ‡å®šé•·åº¦çš„æ¸¬è©¦éŸ³æª”"""
    sample_rate = 16000
    
    # å‰µå»ºæ¨¡æ“¬èªéŸ³çš„éŸ³æª”ï¼ˆæœ‰å¤šå€‹é »ç‡æˆåˆ†ï¼‰
    t = np.linspace(0, duration_seconds, sample_rate * duration_seconds)
    
    # çµ„åˆå¤šå€‹é »ç‡æ¨¡æ“¬èªéŸ³ç‰¹å¾µ
    audio = np.zeros_like(t)
    for freq in [150, 300, 600, 1200]:  # æ¨¡æ“¬äººè²é »ç‡
        audio += 0.1 * np.sin(2 * np.pi * freq * t)
    
    # æ·»åŠ ä¸€äº›éš¨æ©Ÿå™ªéŸ³
    audio += 0.05 * np.random.normal(0, 1, len(t))
    
    # æ­¸ä¸€åŒ–ä¸¦è½‰æ›ç‚º 16-bit
    audio = audio / np.max(np.abs(audio)) * 0.8
    audio_int16 = (audio * 32767).astype(np.int16)
    
    # ä¿å­˜ç‚ºè‡¨æ™‚ WAV æª”æ¡ˆ
    temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
    with wave.open(temp_file.name, 'w') as wav_file:
        wav_file.setnchannels(1)  # å–®è²é“
        wav_file.setsampwidth(2)  # 16-bit
        wav_file.setframerate(sample_rate)
        wav_file.writeframes(audio_int16.tobytes())
    
    return temp_file.name

def test_project_configuration():
    """æ¸¬è©¦é …ç›®ä¸­çš„ faster-whisper é…ç½®"""
    print("ğŸš€ æ¸¬è©¦ VidScript é …ç›®é…ç½®...")
    
    try:
        from faster_whisper import WhisperModel
        import torch
        
        # æª¢æŸ¥ç’°å¢ƒ
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        
        # æ¸¬è©¦ä¸åŒçš„éŸ³æª”é•·åº¦
        test_durations = [5, 15, 30, 60]  # ç§’
        
        # æ¸¬è©¦é…ç½®ï¼ˆèˆ‡é …ç›®ä¸­ä½¿ç”¨çš„ç›¸åŒï¼‰
        configs = [
            {
                "name": "CPU æ¨¡å¼", 
                "device": "cpu", 
                "compute_type": "int8",
                "description": "CPU åŸºæº–æ¸¬è©¦"
            }
        ]
        
        # æ·»åŠ  GPU é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            configs.extend([
                {
                    "name": "GPU Float16", 
                    "device": "cuda", 
                    "compute_type": "float16",
                    "description": "GPU é«˜ç²¾åº¦æ¨¡å¼"
                },
                {
                    "name": "GPU Int8_Float16 (æ¨è–¦)", 
                    "device": "cuda", 
                    "compute_type": "int8_float16",
                    "description": "GPU æœ€ä½³æ€§èƒ½æ¨¡å¼"
                }
            ])
        
        # å­˜å„²çµæœ
        all_results = {}
        
        for config in configs:
            print(f"\n{'='*50}")
            print(f"ğŸ”§ æ¸¬è©¦é…ç½®: {config['name']}")
            print(f"ğŸ“ æè¿°: {config['description']}")
            print(f"âš™ï¸ è¨­å‚™: {config['device']}, è¨ˆç®—é¡å‹: {config['compute_type']}")
            
            try:
                # è¼‰å…¥æ¨¡å‹
                print("\nğŸ”„ è¼‰å…¥æ¨¡å‹...")
                start_time = time.time()
                
                model = WhisperModel(
                    "small",  # ä½¿ç”¨ small æ¨¡å‹ï¼ˆèˆ‡é …ç›®é…ç½®ä¸€è‡´ï¼‰
                    device=config["device"],
                    compute_type=config["compute_type"]
                )
                
                load_time = time.time() - start_time
                print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.2f} ç§’")
                
                # GPU è¨˜æ†¶é«”ç›£æ§
                if config["device"] == "cuda":
                    allocated = torch.cuda.memory_allocated(0) / 1024**3
                    reserved = torch.cuda.memory_reserved(0) / 1024**3
                    print(f"ğŸ“Š GPU è¨˜æ†¶é«” - å·²åˆ†é…: {allocated:.3f}GB, ä¿ç•™: {reserved:.3f}GB")
                
                # æ¸¬è©¦ä¸åŒé•·åº¦çš„éŸ³æª”
                config_results = {"load_time": load_time, "tests": {}}
                
                for duration in test_durations:
                    print(f"\nğŸµ æ¸¬è©¦ {duration} ç§’éŸ³æª”...")
                    
                    # å‰µå»ºæ¸¬è©¦éŸ³æª”
                    audio_file = create_test_audio(duration)
                    
                    try:
                        # åŸ·è¡Œè½‰éŒ„
                        start_time = time.time()
                        segments, info = model.transcribe(
                            audio_file,
                            beam_size=5,
                            language="zh"  # æŒ‡å®šä¸­æ–‡é¿å…èªè¨€æª¢æ¸¬
                        )
                        
                        # æ¶ˆè²»ç”Ÿæˆå™¨ç¢ºä¿å®Œæ•´è½‰éŒ„
                        segment_list = list(segments)
                        transcribe_time = time.time() - start_time
                        
                        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
                        speed_factor = duration / transcribe_time  # é€Ÿåº¦å€æ•¸
                        rtf = transcribe_time / duration  # å¯¦æ™‚å€æ•¸
                        
                        # ä¿å­˜çµæœ
                        result = {
                            "duration": duration,
                            "transcribe_time": transcribe_time,
                            "speed_factor": speed_factor,
                            "rtf": rtf,
                            "segments": len(segment_list),
                            "language": info.language,
                            "language_probability": info.language_probability
                        }
                        
                        config_results["tests"][duration] = result
                        
                        # é¡¯ç¤ºçµæœ
                        print(f"   â±ï¸ è½‰éŒ„æ™‚é–“: {transcribe_time:.2f} ç§’")
                        print(f"   ğŸš€ é€Ÿåº¦å€æ•¸: {speed_factor:.1f}x (æ¯”å¯¦æ™‚å¿« {speed_factor:.1f} å€)")
                        print(f"   ğŸ“ˆ å¯¦æ™‚å€æ•¸: {rtf:.3f} ({'âœ… æ¯”å¯¦æ™‚å¿«' if rtf < 1.0 else 'âš ï¸ æ¯”å¯¦æ™‚æ…¢'})")
                        print(f"   ğŸ—£ï¸ æª¢æ¸¬èªè¨€: {info.language} (ç½®ä¿¡åº¦: {info.language_probability:.2f})")
                        print(f"   ğŸ“ è½‰éŒ„æ®µè½: {len(segment_list)} å€‹")
                        
                    finally:
                        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                        if os.path.exists(audio_file):
                            os.remove(audio_file)
                
                all_results[config["name"]] = config_results
                
                # æ¸…ç†æ¨¡å‹
                del model
                if config["device"] == "cuda":
                    torch.cuda.empty_cache()
                
                print(f"âœ… {config['name']} æ¸¬è©¦å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {config['name']} æ¸¬è©¦å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return all_results
        
    except ImportError as e:
        print(f"âŒ å°å…¥éŒ¯èª¤: {e}")
        print("è«‹ç¢ºèª faster-whisper å·²æ­£ç¢ºå®‰è£")
        return None
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_performance_results(results):
    """åˆ†ææ€§èƒ½æ¸¬è©¦çµæœ"""
    if not results:
        print("âŒ æ²’æœ‰æ¸¬è©¦çµæœå¯åˆ†æ")
        return
    
    print(f"\n{'='*60}")
    print("ğŸ“Š æ€§èƒ½åˆ†æå ±å‘Š")
    print(f"{'='*60}")
    
    # å‰µå»ºæ€§èƒ½æ¯”è¼ƒè¡¨
    print(f"\nğŸ“ˆ è½‰éŒ„æ€§èƒ½å°æ¯”è¡¨:")
    print(f"{'é…ç½®':<20} {'éŸ³æª”é•·åº¦':<10} {'è½‰éŒ„æ™‚é–“':<10} {'é€Ÿåº¦å€æ•¸':<10} {'å¯¦æ™‚å€æ•¸':<10}")
    print("-" * 70)
    
    best_config = None
    best_avg_speed = 0
    
    for config_name, config_data in results.items():
        speeds = []
        
        for duration, test_result in config_data["tests"].items():
            speed_factor = test_result["speed_factor"]
            transcribe_time = test_result["transcribe_time"]
            rtf = test_result["rtf"]
            
            speeds.append(speed_factor)
            
            print(f"{config_name:<20} {duration}ç§’{'':<6} {transcribe_time:<9.2f}ç§’ {speed_factor:<9.1f}x {rtf:<9.3f}")
        
        # è¨ˆç®—å¹³å‡é€Ÿåº¦
        avg_speed = sum(speeds) / len(speeds) if speeds else 0
        if avg_speed > best_avg_speed:
            best_avg_speed = avg_speed
            best_config = config_name
        
        print(f"{'':<20} {'å¹³å‡':<10} {'':<10} {avg_speed:<9.1f}x")
        print("-" * 70)
    
    # æ¨è–¦é…ç½®
    print(f"\nğŸ† æ¨è–¦é…ç½®åˆ†æ:")
    
    if best_config:
        print(f"ğŸ¥‡ æœ€ä½³æ€§èƒ½: {best_config} (å¹³å‡é€Ÿåº¦ {best_avg_speed:.1f}x)")
        
        # çµ¦å‡ºå…·é«”çš„ VidScript é…ç½®å»ºè­°
        if "GPU" in best_config and "Int8_Float16" in best_config:
            print(f"\nğŸ’¡ VidScript é …ç›®å»ºè­°é…ç½®:")
            print(f"```python")
            print(f"device = 'cuda'")
            print(f"compute_type = 'int8_float16'")
            print(f"```")
            print(f"ğŸ¯ é€™æ˜¯ç›®å‰æ¸¬è©¦ä¸­æ€§èƒ½æœ€ä½³çš„é…ç½®ï¼")
        elif "GPU" in best_config:
            print(f"\nğŸ’¡ VidScript é …ç›®å»ºè­°é…ç½®:")
            print(f"```python")
            print(f"device = 'cuda'")
            print(f"compute_type = 'float16'")
            print(f"```")
        else:
            print(f"\nğŸ’¡ VidScript é …ç›®å»ºè­°é…ç½®:")
            print(f"```python")
            print(f"device = 'cpu'")
            print(f"compute_type = 'int8'")
            print(f"```")
            print(f"âš ï¸ GPU å¯èƒ½ä¸å¯ç”¨æˆ–æ€§èƒ½ä¸ä½³ï¼Œå»ºè­°ä½¿ç”¨ CPU é…ç½®")
    
    # æ€§èƒ½è©•ä¼°
    print(f"\nğŸ“‹ æ€§èƒ½è©•ä¼°:")
    
    cpu_config = None
    gpu_config = None
    
    for config_name, config_data in results.items():
        if "CPU" in config_name:
            cpu_config = config_data
        elif "æ¨è–¦" in config_name or "Int8_Float16" in config_name:
            gpu_config = config_data
    
    if cpu_config and gpu_config:
        # æ¯”è¼ƒ 30 ç§’éŸ³æª”çš„æ€§èƒ½
        if 30 in cpu_config["tests"] and 30 in gpu_config["tests"]:
            cpu_time = cpu_config["tests"][30]["transcribe_time"]
            gpu_time = gpu_config["tests"][30]["transcribe_time"]
            improvement = cpu_time / gpu_time
            
            print(f"ğŸš€ GPU ç›¸å°æ–¼ CPU çš„æå‡: {improvement:.1f}x")
            if improvement > 5:
                print(f"   ğŸ”¥ GPU åŠ é€Ÿæ•ˆæœé¡¯è‘—ï¼å»ºè­°ä½¿ç”¨ GPU é…ç½®")
            elif improvement > 2:
                print(f"   âœ… GPU åŠ é€Ÿæ•ˆæœä¸éŒ¯ï¼Œæ¨è–¦ä½¿ç”¨ GPU é…ç½®")
            else:
                print(f"   âš ï¸ GPU åŠ é€Ÿæ•ˆæœä¸€èˆ¬ï¼Œå¯è€ƒæ…® CPU é…ç½®")
    
    # å¯¦æ™‚æ€§åˆ†æ
    print(f"\nâ±ï¸ å¯¦æ™‚æ€§åˆ†æ:")
    print(f"   å¯¦æ™‚å€æ•¸ < 0.1: ğŸ”¥ æ¥µå¿« (æ¨è–¦ç”¨æ–¼å¤§é‡è™•ç†)")
    print(f"   å¯¦æ™‚å€æ•¸ < 0.5: âœ… å¾ˆå¿« (é©ç”¨æ–¼å¤§å¤šæ•¸å ´æ™¯)")
    print(f"   å¯¦æ™‚å€æ•¸ < 1.0: ğŸ“± å¯¦æ™‚ (é©ç”¨æ–¼å³æ™‚æ‡‰ç”¨)")
    print(f"   å¯¦æ™‚å€æ•¸ > 1.0: âš ï¸ è¼ƒæ…¢ (å¯èƒ½éœ€è¦å„ªåŒ–)")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸƒâ€â™‚ï¸ VidScript Faster-Whisper é€Ÿåº¦æ¸¬è©¦")
    print("ğŸ¯ æ¸¬è©¦ç•¶å‰é …ç›®é…ç½®çš„å¯¦éš›æ€§èƒ½")
    print(f"ğŸ“ æ¸¬è©¦ç›®éŒ„: {os.getcwd()}\n")
    
    try:
        # åŸ·è¡Œæ€§èƒ½æ¸¬è©¦
        results = test_project_configuration()
        
        if results:
            # åˆ†æçµæœ
            analyze_performance_results(results)
            
            print(f"\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")
            print(f"\nğŸ’¡ å¦‚ä½•æ‡‰ç”¨æ¸¬è©¦çµæœ:")
            print(f"1. æ ¹æ“šæ¨è–¦é…ç½®ä¿®æ”¹ VidScript çš„æ¨¡å‹è¨­å®š")
            print(f"2. åœ¨ src/services/video_processor.py ä¸­èª¿æ•´åƒæ•¸")
            print(f"3. è§€å¯Ÿå¯¦éš›ä½¿ç”¨ä¸­çš„æ€§èƒ½æå‡")
            print(f"4. å¦‚æœ GPU æ€§èƒ½ä¸ä½³ï¼Œå¯ä»¥å›é€€åˆ° CPU é…ç½®")
        else:
            print(f"âŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒé…ç½®")
    
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
